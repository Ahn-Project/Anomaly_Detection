# -*- coding: utf-8 -*-
"""train (similarity_based)_with real.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1La4YIl7xThrTnKYfiStsXkiDYJ5_fdFp
"""


# from __future__ import print_function, division
from model import resnet
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import numpy as np
import pandas as pd
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy
import argparse


##########################################################
def load_trainset(arg_data):
    # real image 데이터 로드
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize(224),
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),
            # transforms.RandomRotation([45,90,180,270]),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    }

    root_dir = './data/DAGM2007/{}/'.format(arg_data)
    dataset = {x: datasets.ImageFolder(os.path.join(root_dir, x),
                                       transform=data_transforms[x]) for x in ['train', 'val']}

    data_loader = {x: torch.utils.data.DataLoader(dataset[x],
                                                  batch_size=4,
                                                  shuffle=True,
                                                  num_workers=4) for x in ['train', 'val']}

    return dataset, data_loader


def train_model(data_loader, dataset_sizes, model, criterion, optimizer, scheduler, num_epochs, device):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    fvs, label_epoch = [], []  # 추가 부분
    for epoch in range(num_epochs):
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_correct = 0
            for inputs, labels in data_loader[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()
                with torch.set_grad_enabled(phase=='train'):
                    outputs,fv = model(inputs)  # 수정 부분
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    if phase == 'val' and 1<=epoch<3:
                        fvs.extend(fv) # 추가 부분
                        label_epoch.extend(labels) # 추가 부분

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item()*inputs.size(0)
                running_correct += torch.sum(preds==labels.data)

            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_correct / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                  phase, epoch_loss, epoch_acc))

            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
      time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # 가장 나은 모델 가중치를 불러옴
    model.load_state_dict(best_model_wts)
    return model, best_acc, fvs, label_epoch


def save_model(data_loader, dataset_sizes, n_classes, device, arg_data, version, num_epochs):
    # 모델 학습
    model_ft = resnet.resnet18(pretrained=True)
    num_ftrs = model_ft.fc.in_features
    model_ft.fc = nn.Linear(num_ftrs, n_classes)
    model_ft = model_ft.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)
    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

    model_ft, best_acc, fvs, label_epoch = train_model(data_loader, dataset_sizes,
                                                       model_ft, criterion, optimizer_ft, exp_lr_scheduler,
                                                       num_epochs, device)

    ################
    # weight 저장
    wts_save_path = './weights/'
    torch.save(model_ft.state_dict(), wts_save_path+'weights_{}_ver{} (epochs={}).pth'.format(arg_data, version, num_epochs))
    return fvs, label_epoch


def save_fvs(data_loader, dataset_sizes, class_names, n_classes, device, arg_data, version, num_epochs):
    fvs, label_epoch = save_model(data_loader, dataset_sizes, n_classes, device, arg_data, version, num_epochs)

    # fvs 저장
    fvs_list = list(map(lambda x: fvs[x].tolist(), range(len(fvs))))  # 추가 부분
    label_item = list(map(lambda x: label_epoch[x].item(), range(len(label_epoch))))  # 추가 부분

    fvs_array = np.array(fvs_list[:])
    label_array = np.array(label_item[:])

    fvs_save_path = './fvs/'
    dir_path = fvs_save_path + 'fvs_{}_ver{}'.format(arg_data, version)
    if not os.path.isdir(dir_path):
        os.mkdir(dir_path)

    np.save(os.path.join(dir_path, 'fvs_{}_ver{}'.format(arg_data, version)), fvs_array)
    np.save(os.path.join(dir_path, 'label_{}_ver{}'.format(arg_data, version)), label_array)

    # label 저장
    class_names_df = pd.DataFrame(class_names, columns=['label'])
    class_names_df.to_csv(os.path.join(dir_path, 'class_names.csv'), index=False)


def run():
      torch.multiprocessing.freeze_support()
      print('loop')




